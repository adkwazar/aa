{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c13646-4a55-4a6b-bade-9eb21256b07f",
   "metadata": {},
   "source": [
    "<h4> AA, SS --> 1-9</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f000c96b-11c2-4e9c-8057-e5ca6ec91a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sekw.txt\", \"r\") as f:\n",
    "    sekwencje = [linia.strip() for linia in f]\n",
    "\n",
    "with open(\"drugo.txt\", \"r\") as f:\n",
    "    drugorzedowe = [linia.strip() for linia in f]\n",
    "\n",
    "with open(\"wart.txt\", \"r\") as f:\n",
    "    wartosci = [[int(c) for c in linia.strip()] for linia in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d05e902-6429-4326-bfc2-94b99d2caa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for i in range(557): #poczatek danych\n",
    "    raw_data.append((sekwencje[i], drugorzedowe[i], wartosci[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8d75ba-0475-44fe-8e5a-dccc914355ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Dane\n",
    "# =========================\n",
    "\n",
    "vocab = list(set(\"\".join(sekwencje)))+list(set(\"\".join(drugorzedowe)))\n",
    "char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "num_classes = 10 #0-9\n",
    "PAD_IDX = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def encode(seq):\n",
    "    return torch.tensor([char2idx[c] for c in seq], dtype=torch.long)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.samples = []\n",
    "        for s1, s2, labels in data:\n",
    "            self.samples.append((\n",
    "                encode(s1),\n",
    "                encode(s2),\n",
    "                torch.tensor(labels)\n",
    "            ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "dataset = SequenceDataset(raw_data)\n",
    "\n",
    "# =========================\n",
    "# 2. Podział train/val/test\n",
    "# =========================\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 3. Collate z paddingiem\n",
    "# =========================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seq1_list, seq2_list, labels_list = zip(*batch)\n",
    "    lengths = torch.tensor([len(s) for s in seq1_list])\n",
    "    \n",
    "    seq1_padded = pad_sequence(seq1_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    seq2_padded = pad_sequence(seq2_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return seq1_padded, seq2_padded, labels_padded, lengths\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# =========================\n",
    "# 4. Model\n",
    "# =========================\n",
    "\n",
    "class AdvancedDualRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim*2,\n",
    "            hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim*2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, seq1, seq2, lengths):\n",
    "        emb1 = self.embedding(seq1)\n",
    "        emb2 = self.embedding(seq2)\n",
    "        x = torch.cat([emb1, emb2], dim=-1)\n",
    "        \n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_out, _ = self.rnn(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return self.fc(out)\n",
    "\n",
    "model = AdvancedDualRNN(vocab_size, 32, 64, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# =========================\n",
    "# 5. Funkcja ewaluacji\n",
    "# =========================\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2, labels, lengths in loader:\n",
    "            \n",
    "            seq1 = seq1.to(device)\n",
    "            seq2 = seq2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            outputs = model(seq1, seq2, lengths)\n",
    "            \n",
    "            loss = criterion(\n",
    "                outputs.view(-1, num_classes),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            mask = labels != -100\n",
    "            \n",
    "            correct += (preds[mask] == labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c932998-2e4e-4f97-b78b-63ce0713356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== TEST RESULT ====\n",
      "Test Loss: 2.6932\n",
      "Test Accuracy: 0.0209\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(test_loader)\n",
    "\n",
    "print(\"\\n==== TEST RESULT ====\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39ca0f4-2ef7-4122-8c99-cb27de221edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 1.3888 | Val Loss: 0.8941 | Val Acc: 0.5443\n",
      "Epoch 2/20 | Train Loss: 0.9569 | Val Loss: 0.8625 | Val Acc: 0.5492\n",
      "Epoch 3/20 | Train Loss: 0.9264 | Val Loss: 0.8354 | Val Acc: 0.5971\n",
      "Epoch 4/20 | Train Loss: 0.8948 | Val Loss: 0.8696 | Val Acc: 0.5574\n",
      "Epoch 5/20 | Train Loss: 0.8762 | Val Loss: 0.8155 | Val Acc: 0.6118\n",
      "Epoch 6/20 | Train Loss: 0.8393 | Val Loss: 0.8272 | Val Acc: 0.6040\n",
      "Epoch 7/20 | Train Loss: 0.8142 | Val Loss: 0.7956 | Val Acc: 0.6281\n",
      "Epoch 8/20 | Train Loss: 0.7729 | Val Loss: 0.7733 | Val Acc: 0.6401\n",
      "Epoch 9/20 | Train Loss: 0.7508 | Val Loss: 0.7940 | Val Acc: 0.6335\n",
      "Epoch 10/20 | Train Loss: 0.7138 | Val Loss: 0.7576 | Val Acc: 0.6586\n",
      "Epoch 11/20 | Train Loss: 0.6744 | Val Loss: 0.7615 | Val Acc: 0.6600\n",
      "Epoch 12/20 | Train Loss: 0.6617 | Val Loss: 0.7648 | Val Acc: 0.6605\n",
      "Epoch 13/20 | Train Loss: 0.6326 | Val Loss: 0.7985 | Val Acc: 0.6624\n",
      "Epoch 14/20 | Train Loss: 0.6115 | Val Loss: 0.7578 | Val Acc: 0.6721\n",
      "Epoch 15/20 | Train Loss: 0.5922 | Val Loss: 0.7690 | Val Acc: 0.6762\n",
      "Epoch 16/20 | Train Loss: 0.5691 | Val Loss: 0.7805 | Val Acc: 0.6721\n",
      "Epoch 17/20 | Train Loss: 0.5597 | Val Loss: 0.7852 | Val Acc: 0.6728\n",
      "Epoch 18/20 | Train Loss: 0.5592 | Val Loss: 0.8086 | Val Acc: 0.6696\n",
      "Epoch 19/20 | Train Loss: 0.5543 | Val Loss: 0.7918 | Val Acc: 0.6745\n",
      "Epoch 20/20 | Train Loss: 0.5452 | Val Loss: 0.7933 | Val Acc: 0.6749\n",
      "\n",
      "==== TEST RESULT ====\n",
      "Test Loss: 0.8231\n",
      "Test Accuracy: 0.6866\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6. Trening + walidacja\n",
    "# =========================\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for seq1, seq2, labels, lengths in train_loader:\n",
    "        \n",
    "        seq1 = seq1.to(device)\n",
    "        seq2 = seq2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(seq1, seq2, lengths)\n",
    "        \n",
    "        loss = criterion(\n",
    "            outputs.view(-1, num_classes),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 7. Test końcowy\n",
    "# =========================\n",
    "\n",
    "test_loss, test_acc = evaluate(test_loader)\n",
    "\n",
    "print(\"\\n==== TEST RESULT ====\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c8a3d5-e1c8-456b-ae5d-e0681f7bb55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G': 0.6606822262118492, 'A': 0.5659340659340659, 'S': 0.7649484536082474, 'R': 0.8248847926267281, 'L': 0.5804347826086956, 'I': 0.6245847176079734, 'M': 1.0, 'P': 0.6995192307692307, 'K': 0.5384615384615384, 'T': 0.6966824644549763, 'V': 0.5940860215053764, 'H': 0.6877828054298643, 'F': 0.7612456747404844, 'E': 0.5882352941176471, 'D': 0.7645429362880887, 'N': 0.6988636363636364, 'Q': 0.7947019867549668, 'Y': 0.6883116883116883, 'W': 1.0, 'C': 0.692063492063492}\n"
     ]
    }
   ],
   "source": [
    "idx2char = {v: k for k, v in char2idx.items()}\n",
    "\n",
    "def letter_accuracy(loader):\n",
    "    model.eval()\n",
    "    \n",
    "    char_stats = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq1, seq2, labels, lengths in loader:\n",
    "            \n",
    "            seq1 = seq1.to(device)\n",
    "            seq2 = seq2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            outputs = model(seq1, seq2, lengths)\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            \n",
    "            for b in range(seq1.size(0)):          # po batchu\n",
    "                length = lengths[b].item()\n",
    "                \n",
    "                for i in range(length):           # bez paddingu\n",
    "                    \n",
    "                    char_idx = seq1[b, i].item()\n",
    "                    char = idx2char[char_idx]\n",
    "                    \n",
    "                    correct = preds[b, i].item() == labels[b, i].item()\n",
    "                    \n",
    "                    char_stats[char][\"total\"] += 1\n",
    "                    if correct:\n",
    "                        char_stats[char][\"correct\"] += 1\n",
    "    \n",
    "    # liczymy accuracy\n",
    "    results = {}\n",
    "    for char, stats in char_stats.items():\n",
    "        results[char] = stats[\"correct\"] / stats[\"total\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_letter_acc = letter_accuracy(test_loader)\n",
    "print(test_letter_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6429c7b-7e06-4f54-9229-b4b7bf3b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"GRU_aa_SS.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
